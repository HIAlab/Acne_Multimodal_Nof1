{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LinZgQ6agzm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from torchvision.transforms import ToTensor, Resize, Compose\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puNP45YpWnWb",
        "outputId": "65641501-f78b-405a-d1b1-30299aa5b64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trans = Compose([\n",
        "    Resize((224, 224)),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path):\n",
        "        self.xs = []\n",
        "        self.ys = []\n",
        "        for imgname in tqdm(os.listdir(path)):\n",
        "            if imgname[5] not in '01':\n",
        "                continue\n",
        "            self.xs.append(trans(Image.open(path / imgname)))\n",
        "            self.ys.append(torch.tensor(float(imgname[5] != '0')))\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        return self.xs[i], self.ys[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.xs)"
      ],
      "metadata": {
        "id": "gmnNP7WSccg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "IMAGE_FOLDER = Path('/content/drive/Shareddrives/AC297R/Classification/JPEGImages')\n",
        "dataset = ImageDataset(IMAGE_FOLDER)"
      ],
      "metadata": {
        "id": "uUftCxFmdhSK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f899182b-b303-472a-af0a-9680fd005fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1457/1457 [03:34<00:00,  6.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_indices = np.random.choice(len(dataset), int(len(dataset) * 0.7), replace=False)\n",
        "valid_indices = [i for i in range(len(dataset)) if i not in train_indices]\n",
        "dataset_train = torch.utils.data.Subset(dataset, train_indices)\n",
        "dataset_valid = torch.utils.data.Subset(dataset, valid_indices)\n",
        "loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE)#, shuffle=True, num_workers=2)\n",
        "loader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)#, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "zeDRPKL9YCQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset_train), len(dataset_valid))"
      ],
      "metadata": {
        "id": "3UUVgOmPeLsQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00638334-e3ed-4f0b-e84d-e3b9bba53854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "793 341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, pretrained_net, last_shape=1000):\n",
        "        super().__init__()\n",
        "        self.pretrained_net = pretrained_net\n",
        "        '''\n",
        "        for p in self.pretrained_net.parameters():\n",
        "            p.requires_grad = False\n",
        "        \n",
        "        for p in list(self.pretrained_net.parameters())[-6:]:\n",
        "            p.requires_grad = True\n",
        "        '''\n",
        "        for p in list(self.pretrained_net.parameters()):\n",
        "            p.requires_grad = True\n",
        "\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.fc1 = torch.nn.Linear(last_shape, 256)\n",
        "        self.fc2 = torch.nn.Linear(256, 32)\n",
        "        self.fc3 = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(self.relu(self.pretrained_net(x)))\n",
        "        x = self.fc2(self.relu(x))\n",
        "        x = self.fc3(self.relu(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "model = Net(resnet50(weights=ResNet50_Weights.IMAGENET1K_V2), 1000)\n",
        "model.to(DEVICE)\n",
        "\n",
        "for name, p in model.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5nTZiXXeh_s",
        "outputId": "c7ecd5e3-90c7-48ef-a92f-38a28582724e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pretrained_net.conv1.weight torch.Size([64, 3, 7, 7])\n",
            "pretrained_net.bn1.weight torch.Size([64])\n",
            "pretrained_net.bn1.bias torch.Size([64])\n",
            "pretrained_net.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
            "pretrained_net.layer1.0.bn1.weight torch.Size([64])\n",
            "pretrained_net.layer1.0.bn1.bias torch.Size([64])\n",
            "pretrained_net.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
            "pretrained_net.layer1.0.bn2.weight torch.Size([64])\n",
            "pretrained_net.layer1.0.bn2.bias torch.Size([64])\n",
            "pretrained_net.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
            "pretrained_net.layer1.0.bn3.weight torch.Size([256])\n",
            "pretrained_net.layer1.0.bn3.bias torch.Size([256])\n",
            "pretrained_net.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n",
            "pretrained_net.layer1.0.downsample.1.weight torch.Size([256])\n",
            "pretrained_net.layer1.0.downsample.1.bias torch.Size([256])\n",
            "pretrained_net.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
            "pretrained_net.layer1.1.bn1.weight torch.Size([64])\n",
            "pretrained_net.layer1.1.bn1.bias torch.Size([64])\n",
            "pretrained_net.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
            "pretrained_net.layer1.1.bn2.weight torch.Size([64])\n",
            "pretrained_net.layer1.1.bn2.bias torch.Size([64])\n",
            "pretrained_net.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
            "pretrained_net.layer1.1.bn3.weight torch.Size([256])\n",
            "pretrained_net.layer1.1.bn3.bias torch.Size([256])\n",
            "pretrained_net.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
            "pretrained_net.layer1.2.bn1.weight torch.Size([64])\n",
            "pretrained_net.layer1.2.bn1.bias torch.Size([64])\n",
            "pretrained_net.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
            "pretrained_net.layer1.2.bn2.weight torch.Size([64])\n",
            "pretrained_net.layer1.2.bn2.bias torch.Size([64])\n",
            "pretrained_net.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
            "pretrained_net.layer1.2.bn3.weight torch.Size([256])\n",
            "pretrained_net.layer1.2.bn3.bias torch.Size([256])\n",
            "pretrained_net.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
            "pretrained_net.layer2.0.bn1.weight torch.Size([128])\n",
            "pretrained_net.layer2.0.bn1.bias torch.Size([128])\n",
            "pretrained_net.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
            "pretrained_net.layer2.0.bn2.weight torch.Size([128])\n",
            "pretrained_net.layer2.0.bn2.bias torch.Size([128])\n",
            "pretrained_net.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
            "pretrained_net.layer2.0.bn3.weight torch.Size([512])\n",
            "pretrained_net.layer2.0.bn3.bias torch.Size([512])\n",
            "pretrained_net.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
            "pretrained_net.layer2.0.downsample.1.weight torch.Size([512])\n",
            "pretrained_net.layer2.0.downsample.1.bias torch.Size([512])\n",
            "pretrained_net.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
            "pretrained_net.layer2.1.bn1.weight torch.Size([128])\n",
            "pretrained_net.layer2.1.bn1.bias torch.Size([128])\n",
            "pretrained_net.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
            "pretrained_net.layer2.1.bn2.weight torch.Size([128])\n",
            "pretrained_net.layer2.1.bn2.bias torch.Size([128])\n",
            "pretrained_net.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
            "pretrained_net.layer2.1.bn3.weight torch.Size([512])\n",
            "pretrained_net.layer2.1.bn3.bias torch.Size([512])\n",
            "pretrained_net.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
            "pretrained_net.layer2.2.bn1.weight torch.Size([128])\n",
            "pretrained_net.layer2.2.bn1.bias torch.Size([128])\n",
            "pretrained_net.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
            "pretrained_net.layer2.2.bn2.weight torch.Size([128])\n",
            "pretrained_net.layer2.2.bn2.bias torch.Size([128])\n",
            "pretrained_net.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
            "pretrained_net.layer2.2.bn3.weight torch.Size([512])\n",
            "pretrained_net.layer2.2.bn3.bias torch.Size([512])\n",
            "pretrained_net.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
            "pretrained_net.layer2.3.bn1.weight torch.Size([128])\n",
            "pretrained_net.layer2.3.bn1.bias torch.Size([128])\n",
            "pretrained_net.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
            "pretrained_net.layer2.3.bn2.weight torch.Size([128])\n",
            "pretrained_net.layer2.3.bn2.bias torch.Size([128])\n",
            "pretrained_net.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
            "pretrained_net.layer2.3.bn3.weight torch.Size([512])\n",
            "pretrained_net.layer2.3.bn3.bias torch.Size([512])\n",
            "pretrained_net.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
            "pretrained_net.layer3.0.bn1.weight torch.Size([256])\n",
            "pretrained_net.layer3.0.bn1.bias torch.Size([256])\n",
            "pretrained_net.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "pretrained_net.layer3.0.bn2.weight torch.Size([256])\n",
            "pretrained_net.layer3.0.bn2.bias torch.Size([256])\n",
            "pretrained_net.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "pretrained_net.layer3.0.bn3.weight torch.Size([1024])\n",
            "pretrained_net.layer3.0.bn3.bias torch.Size([1024])\n",
            "pretrained_net.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n",
            "pretrained_net.layer3.0.downsample.1.weight torch.Size([1024])\n",
            "pretrained_net.layer3.0.downsample.1.bias torch.Size([1024])\n",
            "pretrained_net.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "pretrained_net.layer3.1.bn1.weight torch.Size([256])\n",
            "pretrained_net.layer3.1.bn1.bias torch.Size([256])\n",
            "pretrained_net.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "pretrained_net.layer3.1.bn2.weight torch.Size([256])\n",
            "pretrained_net.layer3.1.bn2.bias torch.Size([256])\n",
            "pretrained_net.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "pretrained_net.layer3.1.bn3.weight torch.Size([1024])\n",
            "pretrained_net.layer3.1.bn3.bias torch.Size([1024])\n",
            "pretrained_net.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "pretrained_net.layer3.2.bn1.weight torch.Size([256])\n",
            "pretrained_net.layer3.2.bn1.bias torch.Size([256])\n",
            "pretrained_net.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "pretrained_net.layer3.2.bn2.weight torch.Size([256])\n",
            "pretrained_net.layer3.2.bn2.bias torch.Size([256])\n",
            "pretrained_net.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "pretrained_net.layer3.2.bn3.weight torch.Size([1024])\n",
            "pretrained_net.layer3.2.bn3.bias torch.Size([1024])\n",
            "pretrained_net.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "pretrained_net.layer3.3.bn1.weight torch.Size([256])\n",
            "pretrained_net.layer3.3.bn1.bias torch.Size([256])\n",
            "pretrained_net.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "pretrained_net.layer3.3.bn2.weight torch.Size([256])\n",
            "pretrained_net.layer3.3.bn2.bias torch.Size([256])\n",
            "pretrained_net.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "pretrained_net.layer3.3.bn3.weight torch.Size([1024])\n",
            "pretrained_net.layer3.3.bn3.bias torch.Size([1024])\n",
            "pretrained_net.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "pretrained_net.layer3.4.bn1.weight torch.Size([256])\n",
            "pretrained_net.layer3.4.bn1.bias torch.Size([256])\n",
            "pretrained_net.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "pretrained_net.layer3.4.bn2.weight torch.Size([256])\n",
            "pretrained_net.layer3.4.bn2.bias torch.Size([256])\n",
            "pretrained_net.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "pretrained_net.layer3.4.bn3.weight torch.Size([1024])\n",
            "pretrained_net.layer3.4.bn3.bias torch.Size([1024])\n",
            "pretrained_net.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "pretrained_net.layer3.5.bn1.weight torch.Size([256])\n",
            "pretrained_net.layer3.5.bn1.bias torch.Size([256])\n",
            "pretrained_net.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "pretrained_net.layer3.5.bn2.weight torch.Size([256])\n",
            "pretrained_net.layer3.5.bn2.bias torch.Size([256])\n",
            "pretrained_net.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "pretrained_net.layer3.5.bn3.weight torch.Size([1024])\n",
            "pretrained_net.layer3.5.bn3.bias torch.Size([1024])\n",
            "pretrained_net.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
            "pretrained_net.layer4.0.bn1.weight torch.Size([512])\n",
            "pretrained_net.layer4.0.bn1.bias torch.Size([512])\n",
            "pretrained_net.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
            "pretrained_net.layer4.0.bn2.weight torch.Size([512])\n",
            "pretrained_net.layer4.0.bn2.bias torch.Size([512])\n",
            "pretrained_net.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
            "pretrained_net.layer4.0.bn3.weight torch.Size([2048])\n",
            "pretrained_net.layer4.0.bn3.bias torch.Size([2048])\n",
            "pretrained_net.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n",
            "pretrained_net.layer4.0.downsample.1.weight torch.Size([2048])\n",
            "pretrained_net.layer4.0.downsample.1.bias torch.Size([2048])\n",
            "pretrained_net.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
            "pretrained_net.layer4.1.bn1.weight torch.Size([512])\n",
            "pretrained_net.layer4.1.bn1.bias torch.Size([512])\n",
            "pretrained_net.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
            "pretrained_net.layer4.1.bn2.weight torch.Size([512])\n",
            "pretrained_net.layer4.1.bn2.bias torch.Size([512])\n",
            "pretrained_net.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
            "pretrained_net.layer4.1.bn3.weight torch.Size([2048])\n",
            "pretrained_net.layer4.1.bn3.bias torch.Size([2048])\n",
            "pretrained_net.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
            "pretrained_net.layer4.2.bn1.weight torch.Size([512])\n",
            "pretrained_net.layer4.2.bn1.bias torch.Size([512])\n",
            "pretrained_net.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
            "pretrained_net.layer4.2.bn2.weight torch.Size([512])\n",
            "pretrained_net.layer4.2.bn2.bias torch.Size([512])\n",
            "pretrained_net.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
            "pretrained_net.layer4.2.bn3.weight torch.Size([2048])\n",
            "pretrained_net.layer4.2.bn3.bias torch.Size([2048])\n",
            "pretrained_net.fc.weight torch.Size([1000, 2048])\n",
            "pretrained_net.fc.bias torch.Size([1000])\n",
            "fc1.weight torch.Size([256, 1000])\n",
            "fc1.bias torch.Size([256])\n",
            "fc2.weight torch.Size([32, 256])\n",
            "fc2.bias torch.Size([32])\n",
            "fc3.weight torch.Size([1, 32])\n",
            "fc3.bias torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(model):\n",
        "    acc = 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader_valid:\n",
        "            y_pred = model(x.to(DEVICE))\n",
        "            y_pred = torch.sigmoid(y_pred)\n",
        "            y_pred = (y_pred > 0.5).cpu().numpy().astype(int)\n",
        "            y_true = y.cpu().numpy().astype(int)\n",
        "            # print((y_true == y_pred).shape)\n",
        "            acc += (y_true.flatten() == y_pred.flatten()).sum()\n",
        "            count += len(y)\n",
        "    return acc / count\n",
        "\n",
        "    \n",
        "from torch.optim import Adam\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "loss_fn = BCEWithLogitsLoss()\n",
        "\n",
        "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(epoch)\n",
        "    epoch_loss = 0\n",
        "    for x, y in loader_train:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y.reshape(-1, 1))\n",
        "        epoch_loss = loss.item() * len(y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('valid acc =', compute_accuracy(model))\n",
        "    print('epoch loss =', epoch_loss / len(dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xGoZz0He4h5",
        "outputId": "aae93261-1239-461e-80aa-8ffdbd49aa9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "valid acc = 0.8269794721407625\n",
            "epoch loss = 0.01242908746056666\n",
            "1\n",
            "valid acc = 0.8269794721407625\n",
            "epoch loss = 0.0068210577838635315\n",
            "2\n",
            "valid acc = 0.7419354838709677\n",
            "epoch loss = 0.0011615901922632987\n",
            "3\n",
            "valid acc = 0.7888563049853372\n",
            "epoch loss = 0.0003980774912043644\n",
            "4\n",
            "valid acc = 0.8211143695014663\n",
            "epoch loss = 0.00012744436980055964\n",
            "5\n",
            "valid acc = 0.8035190615835777\n",
            "epoch loss = 0.00021816418149772984\n",
            "6\n",
            "valid acc = 0.8240469208211144\n",
            "epoch loss = 2.2612765166133223e-05\n",
            "7\n",
            "valid acc = 0.8240469208211144\n",
            "epoch loss = 1.596035840272299e-05\n",
            "8\n",
            "valid acc = 0.8093841642228738\n",
            "epoch loss = 4.278606214894005e-05\n",
            "9\n",
            "valid acc = 0.8357771260997068\n",
            "epoch loss = 0.00017069587508640276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_accuracy(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSrQebdYlI_4",
        "outputId": "77badfaf-d8e5-487a-e88c-4f5a80e67031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8357771260997068"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, '/content/drive/Shareddrives/AC297R/acne04_pretrained_resnet50_model.pth')"
      ],
      "metadata": {
        "id": "vhgVWUd70KBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SOCHZpye8FQ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}