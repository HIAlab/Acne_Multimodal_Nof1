{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "pJYOxay1YpwT"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor, Resize, Compose, RandomHorizontalFlip, RandomRotation\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import cm\n",
        "from scipy.stats import ttest_ind"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jII57fEjVmFN",
        "outputId": "4698eee8-d5cd-438e-b3e6-87b25ae1bc96"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/Shareddrives/AC297R/images_new/scores-scaled-combined.csv')\n",
        "df_lmm = df.copy()"
      ],
      "metadata": {
        "id": "HZZlhwFGisM4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_ids = []\n",
        "for i in [1,2,3,4,5]:\n",
        "  s = df_lmm[df_lmm['Id'] == i].sort_values('Timestamp (From Photo)(MMDD-YYYY-HHMMSS)')\n",
        "  sorted_ids.append(s)\n",
        "df_tsorted = pd.concat(sorted_ids, ignore_index=True)"
      ],
      "metadata": {
        "id": "o4Qdke5_Y3pH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def increase_brightness(img, value=30):\n",
        "    value = np.random.randint(low=0, high=30, size=1)[0]\n",
        "    img = np.array(img)\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    h, s, v = cv2.split(hsv)\n",
        "\n",
        "    lim = 255 - value\n",
        "    v[v > lim] = 255\n",
        "    v[v <= lim] += value\n",
        "\n",
        "    final_hsv = cv2.merge((h, s, v))\n",
        "    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "    return Image.fromarray(img.astype('uint8'), 'RGB')"
      ],
      "metadata": {
        "id": "0J30o5_rfTK3"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_DIR = Path('/content/drive/Shareddrives/AC297R/images_new')\n",
        "\n",
        "test_trans = Compose([\n",
        "    transforms.Lambda(increase_brightness),\n",
        "    Resize((224, 224)),\n",
        "    RandomHorizontalFlip(1.0),\n",
        "    RandomRotation([5, 10]),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "train_trans = Compose([\n",
        "    transforms.Lambda(increase_brightness),\n",
        "    Resize((224, 224)),\n",
        "    RandomHorizontalFlip(0.5),\n",
        "    RandomRotation([5, 10]),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "inf_trans = Compose([\n",
        "    Resize((224, 224)),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, inf = False, only_train_trans = True, test_id = 2, test_trans = test_trans, train_trans = train_trans, inf_trans = inf_trans):\n",
        "        self.xs = []\n",
        "        self.ys = []\n",
        "        self.test_id = test_id\n",
        "        self.test_trans = test_trans\n",
        "        self.train_trans = train_trans\n",
        "        self.inf_trans = inf_trans\n",
        "        self.inf = inf\n",
        "\n",
        "        for i, row in tqdm(df.iterrows()):\n",
        "            fn = row['Image Id(Id-Timestamp)']\n",
        "            im = Image.open(IMAGE_DIR / fn[0] / fn)\n",
        "            if self.inf == True:\n",
        "              self.xs.append(inf_trans(im))\n",
        "              self.ys.append(-1)\n",
        "              continue\n",
        "            if only_train_trans == True:\n",
        "              self.xs.append(train_trans(im))\n",
        "            elif row.Id == self.test_id:\n",
        "              self.xs.append(test_trans(im))\n",
        "            else:\n",
        "              self.xs.append(train_trans(im))\n",
        "\n",
        "            self.ys.append(\n",
        "                float(np.sum([\n",
        "                    row.scores_xuliang_post_median,\n",
        "                    row.scores_siqiao_post_median,\n",
        "                    row.scores_joslyn_post_median,\n",
        "                    row.scores_shuheng_post_median,\n",
        "                    row.scores_siqi_post_median\n",
        "                ]) > 2.0)\n",
        "              )\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        return self.xs[i], self.ys[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.xs)"
      ],
      "metadata": {
        "id": "yAOzoVJnkJG7"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each median\n",
        "score_cols = ['scores_xuliang', 'scores_siqiao', 'scores_joslyn', 'scores_shuheng', 'scores_siqi']\n",
        "\n",
        "for col in score_cols:\n",
        "  df_tsorted[col+'_post_median'] = df_tsorted[['Id', col]].groupby(['Id']).apply(lambda x: x[col] >= x[col].median()).reset_index(drop=True).astype(float)\n",
        "\n",
        "df_tsorted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "Fzn76A5vOjpA",
        "outputId": "57f97415-4fd8-4508-e6d2-23d7e64b910a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Timestamp (From Photo)(MMDD-YYYY-HHMMSS)  Id  Image Id(Id-Timestamp)  \\\n",
              "0                           1015-2022-134230   1  1-1015-2022-134230.jpg   \n",
              "1                           1015-2022-174630   1  1-1015-2022-174630.jpg   \n",
              "2                           1015-2022-232914   1  1-1015-2022-232914.jpg   \n",
              "3                           1016-2022-122933   1  1-1016-2022-122933.jpg   \n",
              "4                           1016-2022-180003   1  1-1016-2022-180003.jpg   \n",
              "..                                       ...  ..                     ...   \n",
              "250                         1029-2022-161935   5  5-1029-2022-161935.jpg   \n",
              "251                         1030-2022-012914   5  5-1030-2022-012914.jpg   \n",
              "252                         1030-2022-172704   5  5-1030-2022-172704.jpg   \n",
              "253                         1030-2022-204923   5  5-1030-2022-204923.jpg   \n",
              "254                         1031-2022-021401   5  5-1031-2022-021401.jpg   \n",
              "\n",
              "     Temperature (Â°F) Activity Level (Categorical), i.e none, light, intense  \\\n",
              "0                  78                                               None       \n",
              "1                  78                                               None       \n",
              "2                  78                                               None       \n",
              "3                  77                                               None       \n",
              "4                  77                                               None       \n",
              "..                ...                                                ...       \n",
              "250                75                                              Light       \n",
              "251                75                                               None       \n",
              "252                75                                               None       \n",
              "253                75                                               None       \n",
              "254                75                                               None       \n",
              "\n",
              "     Applied Lotion/Makeup\\n(Boolean) Intervention\\n(Boolean) Unnamed: 7  \\\n",
              "0                               False                   FALSE        NaN   \n",
              "1                               False                   FALSE        NaN   \n",
              "2                               False                   FALSE        NaN   \n",
              "3                               False                   FALSE        NaN   \n",
              "4                               False                   FALSE        NaN   \n",
              "..                                ...                     ...        ...   \n",
              "250                             False                    TRUE        NaN   \n",
              "251                              True                    TRUE        NaN   \n",
              "252                              True                    TRUE        NaN   \n",
              "253                             False                    TRUE        NaN   \n",
              "254                              True                    TRUE        NaN   \n",
              "\n",
              "     scores_siqiao  scores_siqi  scores_shuheng  scores_xuliang  \\\n",
              "0         0.571429         0.60        0.500000        0.142857   \n",
              "1         0.857143         1.00        0.666667        0.571429   \n",
              "2         0.571429         0.40        0.166667        0.428571   \n",
              "3         0.571429         0.80        0.333333        0.714286   \n",
              "4         0.428571         0.00        0.000000        0.285714   \n",
              "..             ...          ...             ...             ...   \n",
              "250       0.500000         0.75        0.500000        0.076923   \n",
              "251       0.833333         1.00        0.500000        0.692308   \n",
              "252       0.500000         1.00        0.750000        0.846154   \n",
              "253       0.333333         0.25        0.500000        0.076923   \n",
              "254       0.500000         1.00        0.500000        0.230769   \n",
              "\n",
              "     scores_joslyn  scores_xuliang_post_median  scores_siqiao_post_median  \\\n",
              "0            0.500                         0.0                        1.0   \n",
              "1            0.500                         1.0                        1.0   \n",
              "2            0.250                         0.0                        1.0   \n",
              "3            0.375                         1.0                        1.0   \n",
              "4            0.250                         0.0                        0.0   \n",
              "..             ...                         ...                        ...   \n",
              "250          0.400                         0.0                        1.0   \n",
              "251          0.800                         1.0                        1.0   \n",
              "252          0.600                         1.0                        1.0   \n",
              "253          0.800                         0.0                        0.0   \n",
              "254          0.400                         0.0                        1.0   \n",
              "\n",
              "     scores_joslyn_post_median  scores_shuheng_post_median  \\\n",
              "0                          1.0                         1.0   \n",
              "1                          1.0                         1.0   \n",
              "2                          0.0                         0.0   \n",
              "3                          1.0                         1.0   \n",
              "4                          0.0                         0.0   \n",
              "..                         ...                         ...   \n",
              "250                        1.0                         1.0   \n",
              "251                        1.0                         1.0   \n",
              "252                        1.0                         1.0   \n",
              "253                        1.0                         1.0   \n",
              "254                        1.0                         1.0   \n",
              "\n",
              "     scores_siqi_post_median  \n",
              "0                        1.0  \n",
              "1                        1.0  \n",
              "2                        0.0  \n",
              "3                        1.0  \n",
              "4                        0.0  \n",
              "..                       ...  \n",
              "250                      1.0  \n",
              "251                      1.0  \n",
              "252                      1.0  \n",
              "253                      0.0  \n",
              "254                      1.0  \n",
              "\n",
              "[255 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-02876be2-8c9d-4690-a1dc-76690cba7ef6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Timestamp (From Photo)(MMDD-YYYY-HHMMSS)</th>\n",
              "      <th>Id</th>\n",
              "      <th>Image Id(Id-Timestamp)</th>\n",
              "      <th>Temperature (Â°F)</th>\n",
              "      <th>Activity Level (Categorical), i.e none, light, intense</th>\n",
              "      <th>Applied Lotion/Makeup\\n(Boolean)</th>\n",
              "      <th>Intervention\\n(Boolean)</th>\n",
              "      <th>Unnamed: 7</th>\n",
              "      <th>scores_siqiao</th>\n",
              "      <th>scores_siqi</th>\n",
              "      <th>scores_shuheng</th>\n",
              "      <th>scores_xuliang</th>\n",
              "      <th>scores_joslyn</th>\n",
              "      <th>scores_xuliang_post_median</th>\n",
              "      <th>scores_siqiao_post_median</th>\n",
              "      <th>scores_joslyn_post_median</th>\n",
              "      <th>scores_shuheng_post_median</th>\n",
              "      <th>scores_siqi_post_median</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1015-2022-134230</td>\n",
              "      <td>1</td>\n",
              "      <td>1-1015-2022-134230.jpg</td>\n",
              "      <td>78</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1015-2022-174630</td>\n",
              "      <td>1</td>\n",
              "      <td>1-1015-2022-174630.jpg</td>\n",
              "      <td>78</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.500</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1015-2022-232914</td>\n",
              "      <td>1</td>\n",
              "      <td>1-1015-2022-232914.jpg</td>\n",
              "      <td>78</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1016-2022-122933</td>\n",
              "      <td>1</td>\n",
              "      <td>1-1016-2022-122933.jpg</td>\n",
              "      <td>77</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.375</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1016-2022-180003</td>\n",
              "      <td>1</td>\n",
              "      <td>1-1016-2022-180003.jpg</td>\n",
              "      <td>77</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>1029-2022-161935</td>\n",
              "      <td>5</td>\n",
              "      <td>5-1029-2022-161935.jpg</td>\n",
              "      <td>75</td>\n",
              "      <td>Light</td>\n",
              "      <td>False</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>1030-2022-012914</td>\n",
              "      <td>5</td>\n",
              "      <td>5-1030-2022-012914.jpg</td>\n",
              "      <td>75</td>\n",
              "      <td>None</td>\n",
              "      <td>True</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.800</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>1030-2022-172704</td>\n",
              "      <td>5</td>\n",
              "      <td>5-1030-2022-172704.jpg</td>\n",
              "      <td>75</td>\n",
              "      <td>None</td>\n",
              "      <td>True</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.600</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>1030-2022-204923</td>\n",
              "      <td>5</td>\n",
              "      <td>5-1030-2022-204923.jpg</td>\n",
              "      <td>75</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>1031-2022-021401</td>\n",
              "      <td>5</td>\n",
              "      <td>5-1031-2022-021401.jpg</td>\n",
              "      <td>75</td>\n",
              "      <td>None</td>\n",
              "      <td>True</td>\n",
              "      <td>TRUE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>255 rows Ã 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02876be2-8c9d-4690-a1dc-76690cba7ef6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-02876be2-8c9d-4690-a1dc-76690cba7ef6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-02876be2-8c9d-4690-a1dc-76690cba7ef6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = [1,2,3,4,5]\n",
        "train_val_ids = [1,2,3,4,5]#[1,3,4,5]\n",
        "inf_ids = [4]#[id for id in all_ids if id not in train_val_ids]\n",
        "\n",
        "train_val_df = pd.concat([df_tsorted[df_tsorted['Id'] == i] for i in train_val_ids], ignore_index=True)\n",
        "inf_df = pd.concat([df_tsorted[df_tsorted['Id'] == i] for i in inf_ids], ignore_index=True)"
      ],
      "metadata": {
        "id": "Sc60pz3FvS67"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_val_dataset = ImageDataset(train_val_df, only_train_trans = False, test_trans=test_trans, train_trans=train_trans)\n",
        "train_indices = np.random.choice(len(train_val_dataset), int(len(train_val_dataset) * 0.8), replace=False)\n",
        "valid_indices = [i for i in range(len(train_val_dataset)) if i not in train_indices]\n",
        "\n",
        "\n",
        "train_dateset = torch.utils.data.Subset(train_val_dataset, train_indices)\n",
        "val_dataset = torch.utils.data.Subset(train_val_dataset, valid_indices)\n",
        "inf_dataset = ImageDataset(inf_df, inf=True, inf_trans = inf_trans)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dateset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "inf_loader = DataLoader(inf_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhJis3ttkjth",
        "outputId": "c68ba6ef-84b7-4f50-e6ef-ab2d16c986a6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "255it [00:54,  4.67it/s]\n",
            "54it [00:07,  6.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, pretrained_net, last_shape=1000):\n",
        "        super().__init__()\n",
        "        self.pretrained_net = pretrained_net\n",
        "        \n",
        "        for p in list(self.pretrained_net.parameters()):\n",
        "            p.requires_grad = True\n",
        "\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.fc = torch.nn.Linear(last_shape, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.relu(self.pretrained_net(x)))\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Net(resnet50(weights=ResNet50_Weights.IMAGENET1K_V2), 1000)\n",
        "model.to(DEVICE)\n",
        "\n",
        "for name, p in model.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm14a0omxxi3",
        "outputId": "236cd87b-6179-484f-ad6c-697c5731b5a8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pretrained_net.conv1.weight torch.Size([64, 3, 7, 7])\n",
            "pretrained_net.bn1.weight torch.Size([64])\n",
            "pretrained_net.bn1.bias torch.Size([64])\n",
            "pretrained_net.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
            "pretrained_net.layer1.0.bn1.weight torch.Size([64])\n",
            "pretrained_net.layer1.0.bn1.bias torch.Size([64])\n",
            "pretrained_net.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
            "pretrained_net.layer1.0.bn2.weight torch.Size([64])\n",
            "pretrained_net.layer1.0.bn2.bias torch.Size([64])\n",
            "pretrained_net.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
            "pretrained_net.layer1.0.bn3.weight torch.Size([256])\n",
            "pretrained_net.layer1.0.bn3.bias torch.Size([256])\n",
            "pretrained_net.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n",
            "pretrained_net.layer1.0.downsample.1.weight torch.Size([256])\n",
            "pretrained_net.layer1.0.downsample.1.bias torch.Size([256])\n",
            "pretrained_net.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
            "pretrained_net.layer1.1.bn1.weight torch.Size([64])\n",
            "pretrained_net.layer1.1.bn1.bias torch.Size([64])\n",
            "pretrained_net.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
            "pretrained_net.layer1.1.bn2.weight torch.Size([64])\n",
            "pretrained_net.layer1.1.bn2.bias torch.Size([64])\n",
            "pretrained_net.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
            "pretrained_net.layer1.1.bn3.weight torch.Size([256])\n",
            "pretrained_net.layer1.1.bn3.bias torch.Size([256])\n",
            "pretrained_net.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
            "pretrained_net.layer1.2.bn1.weight torch.Size([64])\n",
            "pretrained_net.layer1.2.bn1.bias torch.Size([64])\n",
            "pretrained_net.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
            "pretrained_net.layer1.2.bn2.weight torch.Size([64])\n",
            "pretrained_net.layer1.2.bn2.bias torch.Size([64])\n",
            "pretrained_net.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
            "pretrained_net.layer1.2.bn3.weight torch.Size([256])\n",
            "pretrained_net.layer1.2.bn3.bias torch.Size([256])\n",
            "pretrained_net.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
            "pretrained_net.layer2.0.bn1.weight torch.Size([128])\n",
            "pretrained_net.layer2.0.bn1.bias torch.Size([128])\n",
            "pretrained_net.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
            "pretrained_net.layer2.0.bn2.weight torch.Size([128])\n",
            "pretrained_net.layer2.0.bn2.bias torch.Size([128])\n",
            "pretrained_net.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
            "pretrained_net.layer2.0.bn3.weight torch.Size([512])\n",
            "pretrained_net.layer2.0.bn3.bias torch.Size([512])\n",
            "pretrained_net.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
            "pretrained_net.layer2.0.downsample.1.weight torch.Size([512])\n",
            "pretrained_net.layer2.0.downsample.1.bias torch.Size([512])\n",
            "pretrained_net.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
            "pretrained_net.layer2.1.bn1.weight torch.Size([128])\n",
            "pretrained_net.layer2.1.bn1.bias torch.Size([128])\n",
            "pretrained_net.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
            "pretrained_net.layer2.1.bn2.weight torch.Size([128])\n",
            "pretrained_net.layer2.1.bn2.bias torch.Size([128])\n",
            "pretrained_net.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
            "pretrained_net.layer2.1.bn3.weight torch.Size([512])\n",
            "pretrained_net.layer2.1.bn3.bias torch.Size([512])\n",
            "pretrained_net.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
            "pretrained_net.layer2.2.bn1.weight torch.Size([128])\n",
            "pretrained_net.layer2.2.bn1.bias torch.Size([128])\n",
            "pretrained_net.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
            "pretrained_net.layer2.2.bn2.weight torch.Size([128])\n",
            "pretrained_net.layer2.2.bn2.bias torch.Size([128])\n",
            "pretrained_net.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
            "pretrained_net.layer2.2.bn3.weight torch.Size([512])\n",
            "pretrained_net.layer2.2.bn3.bias torch.Size([512])\n",
            "pretrained_net.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
            "pretrained_net.layer2.3.bn1.weight torch.Size([128])\n",
            "pretrained_net.layer2.3.bn1.bias torch.Size([128])\n",
            "pretrained_net.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
            "pretrained_net.layer2.3.bn2.weight torch.Size([128])\n",
            "pretrained_net.layer2.3.bn2.bias torch.Size([128])\n",
            "pretrained_net.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
            "pretrained_net.layer2.3.bn3.weight torch.Size([512])\n",
            "pretrained_net.layer2.3.bn3.bias torch.Size([512])\n",
            "pretrained_net.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
            "pretrained_net.layer3.0.bn1.weight torch.Size([256])\n",
            "pretrained_net.layer3.0.bn1.bias torch.Size([256])\n",
            "pretrained_net.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "pretrained_net.layer3.0.bn2.weight torch.Size([256])\n",
            "pretrained_net.layer3.0.bn2.bias torch.Size([256])\n",
            "pretrained_net.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "pretrained_net.layer3.0.bn3.weight torch.Size([1024])\n",
            "pretrained_net.layer3.0.bn3.bias torch.Size([1024])\n",
            "pretrained_net.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n",
            "pretrained_net.layer3.0.downsample.1.weight torch.Size([1024])\n",
            "pretrained_net.layer3.0.downsample.1.bias torch.Size([1024])\n",
            "pretrained_net.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "pretrained_net.layer3.1.bn1.weight torch.Size([256])\n",
            "pretrained_net.layer3.1.bn1.bias torch.Size([256])\n",
            "pretrained_net.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "pretrained_net.layer3.1.bn2.weight torch.Size([256])\n",
            "pretrained_net.layer3.1.bn2.bias torch.Size([256])\n",
            "pretrained_net.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "pretrained_net.layer3.1.bn3.weight torch.Size([1024])\n",
            "pretrained_net.layer3.1.bn3.bias torch.Size([1024])\n",
            "pretrained_net.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "pretrained_net.layer3.2.bn1.weight torch.Size([256])\n",
            "pretrained_net.layer3.2.bn1.bias torch.Size([256])\n",
            "pretrained_net.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "pretrained_net.layer3.2.bn2.weight torch.Size([256])\n",
            "pretrained_net.layer3.2.bn2.bias torch.Size([256])\n",
            "pretrained_net.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "pretrained_net.layer3.2.bn3.weight torch.Size([1024])\n",
            "pretrained_net.layer3.2.bn3.bias torch.Size([1024])\n",
            "pretrained_net.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "pretrained_net.layer3.3.bn1.weight torch.Size([256])\n",
            "pretrained_net.layer3.3.bn1.bias torch.Size([256])\n",
            "pretrained_net.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "pretrained_net.layer3.3.bn2.weight torch.Size([256])\n",
            "pretrained_net.layer3.3.bn2.bias torch.Size([256])\n",
            "pretrained_net.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "pretrained_net.layer3.3.bn3.weight torch.Size([1024])\n",
            "pretrained_net.layer3.3.bn3.bias torch.Size([1024])\n",
            "pretrained_net.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "pretrained_net.layer3.4.bn1.weight torch.Size([256])\n",
            "pretrained_net.layer3.4.bn1.bias torch.Size([256])\n",
            "pretrained_net.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "pretrained_net.layer3.4.bn2.weight torch.Size([256])\n",
            "pretrained_net.layer3.4.bn2.bias torch.Size([256])\n",
            "pretrained_net.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "pretrained_net.layer3.4.bn3.weight torch.Size([1024])\n",
            "pretrained_net.layer3.4.bn3.bias torch.Size([1024])\n",
            "pretrained_net.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "pretrained_net.layer3.5.bn1.weight torch.Size([256])\n",
            "pretrained_net.layer3.5.bn1.bias torch.Size([256])\n",
            "pretrained_net.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "pretrained_net.layer3.5.bn2.weight torch.Size([256])\n",
            "pretrained_net.layer3.5.bn2.bias torch.Size([256])\n",
            "pretrained_net.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "pretrained_net.layer3.5.bn3.weight torch.Size([1024])\n",
            "pretrained_net.layer3.5.bn3.bias torch.Size([1024])\n",
            "pretrained_net.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
            "pretrained_net.layer4.0.bn1.weight torch.Size([512])\n",
            "pretrained_net.layer4.0.bn1.bias torch.Size([512])\n",
            "pretrained_net.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
            "pretrained_net.layer4.0.bn2.weight torch.Size([512])\n",
            "pretrained_net.layer4.0.bn2.bias torch.Size([512])\n",
            "pretrained_net.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
            "pretrained_net.layer4.0.bn3.weight torch.Size([2048])\n",
            "pretrained_net.layer4.0.bn3.bias torch.Size([2048])\n",
            "pretrained_net.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n",
            "pretrained_net.layer4.0.downsample.1.weight torch.Size([2048])\n",
            "pretrained_net.layer4.0.downsample.1.bias torch.Size([2048])\n",
            "pretrained_net.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
            "pretrained_net.layer4.1.bn1.weight torch.Size([512])\n",
            "pretrained_net.layer4.1.bn1.bias torch.Size([512])\n",
            "pretrained_net.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
            "pretrained_net.layer4.1.bn2.weight torch.Size([512])\n",
            "pretrained_net.layer4.1.bn2.bias torch.Size([512])\n",
            "pretrained_net.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
            "pretrained_net.layer4.1.bn3.weight torch.Size([2048])\n",
            "pretrained_net.layer4.1.bn3.bias torch.Size([2048])\n",
            "pretrained_net.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
            "pretrained_net.layer4.2.bn1.weight torch.Size([512])\n",
            "pretrained_net.layer4.2.bn1.bias torch.Size([512])\n",
            "pretrained_net.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
            "pretrained_net.layer4.2.bn2.weight torch.Size([512])\n",
            "pretrained_net.layer4.2.bn2.bias torch.Size([512])\n",
            "pretrained_net.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
            "pretrained_net.layer4.2.bn3.weight torch.Size([2048])\n",
            "pretrained_net.layer4.2.bn3.bias torch.Size([2048])\n",
            "pretrained_net.fc.weight torch.Size([1000, 2048])\n",
            "pretrained_net.fc.bias torch.Size([1000])\n",
            "fc.weight torch.Size([1, 1000])\n",
            "fc.bias torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import MSELoss\n",
        "\n",
        "\n",
        "def compute_mse_loss(model):\n",
        "  count = 0\n",
        "  loss = 0\n",
        "  loss_fn = MSELoss()\n",
        "  with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "      y_pred = model(x.to(DEVICE))\n",
        "      loss += loss_fn(y, y_pred)\n",
        "      count += len(y)\n",
        "\n",
        "  return loss / count"
      ],
      "metadata": {
        "id": "88lln0KK3Iar"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "def compute_bce_acc(model):\n",
        "  count = 0\n",
        "  acc = 0\n",
        "  loss_fn = BCEWithLogitsLoss()\n",
        "  with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "      y = y.type(torch.FloatTensor).to(DEVICE)\n",
        "      y_pred = model(x.to(DEVICE))\n",
        "      y_pred = torch.sigmoid(y_pred)\n",
        "      y_pred = (y_pred > 0.5).cpu().numpy().astype(int)\n",
        "      y_true = y.cpu().numpy().astype(int)\n",
        "      acc += (y_true.flatten() == y_pred.flatten()).sum()\n",
        "      count += len(y)\n",
        "\n",
        "  return acc / count"
      ],
      "metadata": {
        "id": "m5vHfQhDVE_a"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from torch.nn import MSELoss\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "\n",
        "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
        "loss_fn = BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "1aDFFEwdXzeT"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(15):\n",
        "    print(epoch)\n",
        "    epoch_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        y = y.type(torch.FloatTensor)\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y.reshape(-1, 1))\n",
        "        epoch_loss += loss.item() * len(y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('valid acc =', compute_bce_acc(model))\n",
        "    print('epoch loss =', epoch_loss / len(train_dateset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKvEkc4cyT8R",
        "outputId": "b5fb4acd-b0fd-48bf-c1c3-c588de330493"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "valid acc = 0.6274509803921569\n",
            "epoch loss = 0.6717063772912119\n",
            "1\n",
            "valid acc = 0.7058823529411765\n",
            "epoch loss = 0.5587362635369394\n",
            "2\n",
            "valid acc = 0.6274509803921569\n",
            "epoch loss = 0.5221718689974617\n",
            "3\n",
            "valid acc = 0.5686274509803921\n",
            "epoch loss = 0.46956320662124484\n",
            "4\n",
            "valid acc = 0.6078431372549019\n",
            "epoch loss = 0.4068564199933819\n",
            "5\n",
            "valid acc = 0.7647058823529411\n",
            "epoch loss = 0.2246314614426856\n",
            "6\n",
            "valid acc = 0.7647058823529411\n",
            "epoch loss = 0.27351900701429327\n",
            "7\n",
            "valid acc = 0.6862745098039216\n",
            "epoch loss = 0.3609393887660083\n",
            "8\n",
            "valid acc = 0.6862745098039216\n",
            "epoch loss = 0.429288849234581\n",
            "9\n",
            "valid acc = 0.7843137254901961\n",
            "epoch loss = 0.24575836050744151\n",
            "10\n",
            "valid acc = 0.7647058823529411\n",
            "epoch loss = 0.1668370818974925\n",
            "11\n",
            "valid acc = 0.7058823529411765\n",
            "epoch loss = 0.13466111585205676\n",
            "12\n",
            "valid acc = 0.7450980392156863\n",
            "epoch loss = 0.3353090157695845\n",
            "13\n",
            "valid acc = 0.7058823529411765\n",
            "epoch loss = 0.2584759867658802\n",
            "14\n",
            "valid acc = 0.6666666666666666\n",
            "epoch loss = 0.28766175461750404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score_col = 'scores_resnet50'\n",
        "inf_scores = []\n",
        "for x, _ in inf_loader:\n",
        "  x = x.to(DEVICE)\n",
        "  inf_scores += list(torch.sigmoid(model(x)).detach().cpu().numpy().flatten())\n",
        "\n",
        "inf_df[score_col] = inf_scores\n",
        "\n",
        "inf_df.loc[:, 'Intervention\\n(Boolean)'] = inf_df['Intervention\\n(Boolean)'].apply(lambda s: s.upper().strip())\n",
        "intervention = inf_df['Intervention\\n(Boolean)']\n",
        "x = intervention == 'TRUE'\n",
        "y = intervention == 'FALSE'\n",
        "\n",
        "\n",
        "with_inter = {\n",
        "    idx: inf_df[(inf_df['Id'] == idx) & x][score_col].values\n",
        "    for idx in inf_ids\n",
        "}\n",
        "\n",
        "without_inter = {\n",
        "    idx: inf_df[(inf_df['Id'] == idx) & y][score_col].values\n",
        "    for idx in inf_ids\n",
        "}"
      ],
      "metadata": {
        "id": "F-wTjvfi5L1D"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inf_df.to_csv(\"/content/drive/Shareddrives/AC297R/images_new/shuheng_cnn_inf.csv\")"
      ],
      "metadata": {
        "id": "YgALuXMF5bUX"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aN-dcpF45bW_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}